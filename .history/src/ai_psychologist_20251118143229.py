#!/usr/bin/env python3
"""
AI Psychologist with Long-Term Memory Implementation
"""

import os
import json
import time
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional

# Conditional imports - only import if available
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None

try:
    import chromadb
    from chromadb.utils import embedding_functions
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    chromadb = None
    embedding_functions = None

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    requests = None

from config import Config
from procedural_memory import procedural_memory

class OpenRouterClient:
    def __init__(self, api_key: Optional[str] = None):
        if OPENAI_AVAILABLE:
            self.api_key = api_key or Config.OPENROUTER_API_KEY
            self.client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=self.api_key
            )
        else:
            self.api_key = api_key or Config.OPENROUTER_API_KEY
            self.client = None
    
    def chat_completion(self, messages: List[Dict[str, str]], model: Optional[str] = None) -> Dict[str, Any]:
        """
        Implementation of OpenRouter chat completion API
        """
        if model is None:
            model = Config.DEFAULT_MODEL
            
        # Use real API if available, otherwise fallback to mock
        if OPENAI_AVAILABLE and self.client:
            try:
                response = self.client.chat.completions.create(
                    model=model,
                    messages=messages
                )
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": response.choices[0].message.content
                        }
                    }]
                }
            except Exception as e:
                # Fallback to mock response if API fails
                print(f"Warning: API call failed, using mock response: {e}")
                return self._mock_response(messages)
        else:
            # Use mock response if OpenAI not available
            return self._mock_response(messages)
    
    def _mock_response(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        Mock implementation for fallback when API is not available
        """
        # Simple response generation based on the last user message
        user_message = messages[-1]["content"] if messages else ""
        
        # Simple empathetic responses based on keywords
        responses = {
            "sad": "æˆ‘æ„Ÿè§‰åˆ°ä½ æœ‰äº›éš¾è¿‡ã€‚æœ‰è¿™ç§æ„Ÿè§‰å¾ˆæ­£å¸¸ï¼Œæˆ‘ä¼šé™ªä¼´ä½ ä¸€èµ·é¢å¯¹ã€‚",
            "depress": "æˆ‘å¬åˆ°ä½ æ­£åœ¨ç»å†å›°éš¾æ—¶æœŸã€‚æŠ‘éƒç¡®å®å¾ˆæœ‰æŒ‘æˆ˜æ€§ï¼Œä½†ä½ å¹¶ä¸å­¤å•ã€‚",
            "anxious": "ç„¦è™‘ä¼šè®©äººæ„Ÿåˆ°ä¸å ªé‡è´Ÿã€‚è®©æˆ‘ä»¬ä¸€èµ·æ·±å‘¼å¸ï¼Œæ¢ç´¢ä¸€ä¸‹æ˜¯ä»€ä¹ˆå¼•èµ·äº†è¿™äº›æ„Ÿå—ã€‚",
            "happy": "å¾ˆé«˜å…´å¬åˆ°ä½ æ„Ÿåˆ°ç§¯æï¼æ˜¯ä»€ä¹ˆè®©ä½ æ„Ÿåˆ°å¿«ä¹å‘¢ï¼Ÿ",
            "stress": "å‹åŠ›ç¡®å®ä¼šå¯¹æˆ‘ä»¬äº§ç”Ÿå½±å“ã€‚è®©æˆ‘ä»¬æ‰¾å‡ºå‹åŠ›çš„æ¥æºï¼Œå¹¶æ‰¾åˆ°åº”å¯¹çš„æ–¹æ³•ã€‚",
            "angry": "æ„¤æ€’æ˜¯ä¸€ç§è‡ªç„¶çš„æƒ…ç»ªã€‚è®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹æ˜¯ä»€ä¹ˆè§¦å‘äº†è¿™äº›æ„Ÿå—ï¼Œå¹¶æ‰¾åˆ°å¥åº·çš„æ–¹å¼æ¥è¡¨è¾¾å®ƒä»¬ã€‚",
            "lonely": "æ„Ÿåˆ°å­¤ç‹¬ç¡®å®å¾ˆéš¾å—ã€‚ä½ å¹¶ä¸å­¤å•ï¼Œæˆ‘ä¼šé™ªä¼´ä½ å¹¶æä¾›æ”¯æŒã€‚",
        }
        
        # Default empathetic response
        response_text = "æˆ‘å¬åˆ°äº†ä½ çš„è¯ï¼Œæˆ‘ä¼šé™ªä¼´ä½ ä¸€èµ·é¢å¯¹ã€‚ä½ èƒ½å‘Šè¯‰æˆ‘æ›´å¤šå…³äºä½ çš„æ„Ÿå—å—ï¼Ÿ"
        
        # Try to match keywords in the user message
        user_message_lower = user_message.lower()
        for keyword, response in responses.items():
            if keyword in user_message_lower:
                response_text = response
                break
        
        return {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": response_text
                }
            }]
        }

class OllamaClient:
    def __init__(self, base_url: Optional[str] = None, model: Optional[str] = None):
        self.base_url = base_url or Config.OLLAMA_BASE_URL
        self.model = model or Config.OLLAMA_MODEL
        self.available = REQUESTS_AVAILABLE
    
    def chat_completion(self, messages: List[Dict[str, str]], model: Optional[str] = None) -> Dict[str, Any]:
        """
        Implementation of Ollama chat completion API
        """
        if not self.available:
            return self._mock_response(messages)
        
        model_name = model or self.model
        
        # Format messages for Ollama
        ollama_messages = []
        for msg in messages:
            ollama_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": model_name,
                    "messages": ollama_messages,
                    "stream": False
                },
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": data["message"]["content"]
                        }
                    }]
                }
            else:
                print(f"Warning: Ollama API call failed with status {response.status_code}")
                return self._mock_response(messages)
        except Exception as e:
            print(f"Warning: Ollama API call failed, using mock response: {e}")
            return self._mock_response(messages)
    
    def _mock_response(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        Mock implementation for fallback when Ollama is not available
        """
        # Simple response generation based on the last user message
        user_message = messages[-1]["content"] if messages else ""
        
        # Simple empathetic responses based on keywords
        responses = {
            "sad": "æˆ‘æ„Ÿè§‰åˆ°ä½ æœ‰äº›éš¾è¿‡ã€‚æœ‰è¿™ç§æ„Ÿè§‰å¾ˆæ­£å¸¸ï¼Œæˆ‘ä¼šé™ªä¼´ä½ ä¸€èµ·é¢å¯¹ã€‚",
            "depress": "æˆ‘å¬åˆ°ä½ æ­£åœ¨ç»å†å›°éš¾æ—¶æœŸã€‚æŠ‘éƒç¡®å®å¾ˆæœ‰æŒ‘æˆ˜æ€§ï¼Œä½†ä½ å¹¶ä¸å­¤å•ã€‚",
            "anxious": "ç„¦è™‘ä¼šè®©äººæ„Ÿåˆ°ä¸å ªé‡è´Ÿã€‚è®©æˆ‘ä»¬ä¸€èµ·æ·±å‘¼å¸ï¼Œæ¢ç´¢ä¸€ä¸‹æ˜¯ä»€ä¹ˆå¼•èµ·äº†è¿™äº›æ„Ÿå—ã€‚",
            "happy": "å¾ˆé«˜å…´å¬åˆ°ä½ æ„Ÿåˆ°ç§¯æï¼æ˜¯ä»€ä¹ˆè®©ä½ æ„Ÿåˆ°å¿«ä¹å‘¢ï¼Ÿ",
            "stress": "å‹åŠ›ç¡®å®ä¼šå¯¹æˆ‘ä»¬äº§ç”Ÿå½±å“ã€‚è®©æˆ‘ä»¬æ‰¾å‡ºå‹åŠ›çš„æ¥æºï¼Œå¹¶æ‰¾åˆ°åº”å¯¹çš„æ–¹æ³•ã€‚",
            "angry": "æ„¤æ€’æ˜¯ä¸€ç§è‡ªç„¶çš„æƒ…ç»ªã€‚è®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹æ˜¯ä»€ä¹ˆè§¦å‘äº†è¿™äº›æ„Ÿå—ï¼Œå¹¶æ‰¾åˆ°å¥åº·çš„æ–¹å¼æ¥è¡¨è¾¾å®ƒä»¬ã€‚",
            "lonely": "æ„Ÿåˆ°å­¤ç‹¬ç¡®å®å¾ˆéš¾å—ã€‚ä½ å¹¶ä¸å­¤å•ï¼Œæˆ‘ä¼šé™ªä¼´ä½ å¹¶æä¾›æ”¯æŒã€‚",
        }
        
        # Default empathetic response
        response_text = "æˆ‘å¬åˆ°äº†ä½ çš„è¯ï¼Œæˆ‘ä¼šé™ªä¼´ä½ ä¸€èµ·é¢å¯¹ã€‚ä½ èƒ½å‘Šè¯‰æˆ‘æ›´å¤šå…³äºä½ çš„æ„Ÿå—å—ï¼Ÿ"
        
        # Try to match keywords in the user message
        user_message_lower = user_message.lower()
        for keyword, response in responses.items():
            if keyword in user_message_lower:
                response_text = response
                break
        
        return {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": response_text
                }
            }]
        }

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†"""
    
    def __init__(self):
        self.provider = Config.MODEL_PROVIDER.lower()
        
        if self.provider == "openrouter":
            self.client = OpenRouterClient()
        elif self.provider == "ollama":
            self.client = OllamaClient()
        else:
            # é»˜è®¤ä½¿ç”¨OpenRouter
            self.client = OpenRouterClient()
            self.provider = "openrouter"
    
    def chat_completion(self, messages: List[Dict[str, str]], model: Optional[str] = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„èŠå¤©å®Œæˆæ¥å£
        """
        return self.client.chat_completion(messages, model)

class MemorySystem:
    """Multi-layered memory system for the AI Psychologist"""
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.storage_path = Config.DATA_STORAGE_PATH
        self.user_dir = os.path.join(self.storage_path, user_id)
        
        # Create user directory if it doesn't exist
        os.makedirs(self.user_dir, exist_ok=True)
        
        # Initialize vector database for semantic memory
        self._init_vector_db()
        
        # Initialize memory layers
        self.working_memory = []  # Short-term conversation context
        self.episodic_memory = []  # Time-stamped events and experiences
        self.semantic_memory = {}  # Facts, knowledge, and user profile
        
        # Load existing memories
        self._load_memories()
    
    def _init_vector_db(self):
        """Initialize the vector database for semantic memory"""
        if CHROMA_AVAILABLE:
            try:
                # è®¾ç½®ç¦»çº¿æ¨¡å¼ä»¥é¿å…ç½‘ç»œè¿æ¥é—®é¢˜
                import os
                os.environ['HF_HUB_OFFLINE'] = '1'
                
                chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_PATH)
                # å°è¯•ä½¿ç”¨å›½å†…é•œåƒæº
                try:
                    self.collection = chroma_client.get_or_create_collection(
                        name=f"user_{self.user_id}_memories",
                        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(
                            model_name="all-MiniLM-L6-v2"
                        )
                    )
                    print("âœ“ å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸  å‘é‡æ•°æ®åº“åˆå§‹åŒ–è­¦å‘Š: {e}")
                    print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                    print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                    print("   2. å¦‚æœåœ¨å›½å†…ï¼Œå¯å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡:")
                    print("      set HF_ENDPOINT=https://hf-mirror.com")
                    print("   3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•")
                    print("   4. è¯­ä¹‰è®°å¿†åŠŸèƒ½å°†é™çº§åˆ°æ–‡ä»¶å­˜å‚¨æ¨¡å¼")
                    # é™çº§åˆ°åŸºæœ¬çš„æ–‡æœ¬åŒ¹é…ï¼Œä¸ä½¿ç”¨å‘é‡æ•°æ®åº“
                    self.collection = None
            except Exception as e:
                print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ’¡ è¯­ä¹‰è®°å¿†åŠŸèƒ½å°†é™çº§åˆ°æ–‡ä»¶å­˜å‚¨æ¨¡å¼")
                print("   æ‰€æœ‰æ•°æ®å°†å­˜å‚¨åœ¨ ./data/ ç›®å½•ä¸‹çš„JSONæ–‡ä»¶ä¸­")
                self.collection = None
        else:
            print("âš ï¸  ChromaDBåº“æœªå®‰è£…ï¼Œè¯­ä¹‰è®°å¿†åŠŸèƒ½å°†é™çº§åˆ°æ–‡ä»¶å­˜å‚¨æ¨¡å¼")
            self.collection = None
    
    def _load_memories(self):
        """Load existing memories from storage"""
        # Load semantic memory (user profile and facts)
        semantic_file = os.path.join(self.user_dir, "semantic_memory.json")
        if os.path.exists(semantic_file):
            try:
                with open(semantic_file, 'r', encoding='utf-8') as f:
                    self.semantic_memory = json.load(f)
            except Exception as e:
                print(f"Warning: Could not load semantic memory: {e}")
        
        # Load episodic memory (past conversations)
        episodic_file = os.path.join(self.user_dir, "episodic_memory.json")
        if os.path.exists(episodic_file):
            try:
                with open(episodic_file, 'r', encoding='utf-8') as f:
                    self.episodic_memory = json.load(f)
            except Exception as e:
                print(f"Warning: Could not load episodic memory: {e}")
    
    def save_memories(self):
        """Save all memories to persistent storage"""
        # Save semantic memory
        semantic_file = os.path.join(self.user_dir, "semantic_memory.json")
        try:
            with open(semantic_file, 'w', encoding='utf-8') as f:
                json.dump(self.semantic_memory, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving semantic memory: {e}")
        
        # Save episodic memory
        episodic_file = os.path.join(self.user_dir, "episodic_memory.json")
        try:
            with open(episodic_file, 'w', encoding='utf-8') as f:
                json.dump(self.episodic_memory, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving episodic memory: {e}")
    
    def add_working_memory(self, message: Dict[str, str]):
        """Add a message to working memory"""
        self.working_memory.append({
            "id": str(uuid.uuid4()),
            "timestamp": time.time(),
            "message": message
        })
        
        # Keep only the last N messages in working memory
        if len(self.working_memory) > Config.WORKING_MEMORY_SIZE:
            self.working_memory = self.working_memory[-Config.WORKING_MEMORY_SIZE:]
    
    def add_episodic_memory(self, event: Dict[str, Any]):
        """Add an event to episodic memory"""
        event_entry = {
            "id": str(uuid.uuid4()),
            "timestamp": time.time(),
            "datetime": datetime.now().isoformat(),
            **event
        }
        self.episodic_memory.append(event_entry)
        
        # Add to vector database if available
        if self.collection and "summary" in event:
            try:
                # å°†å¤æ‚å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…å‘é‡æ•°æ®åº“é”™è¯¯
                metadata = {
                    "summary": event["summary"],
                    "timestamp": event_entry["timestamp"],
                    "datetime": event_entry["datetime"]
                }
                
                # å¦‚æœæœ‰äº¤äº’ä¿¡æ¯ï¼Œå°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if "interaction" in event:
                    interaction_str = json.dumps(event["interaction"], ensure_ascii=False)
                    metadata["interaction"] = interaction_str
                
                self.collection.add(
                    documents=[event["summary"]],
                    metadatas=[metadata],  # ä½¿ç”¨ç®€åŒ–åçš„metadata
                    ids=[event_entry["id"]]
                )
            except Exception as e:
                print(f"Warning: Could not add to vector database: {e}")
        
        # Save to persistent storage
        self.save_memories()
    
    def add_time_based_episodic_memory(self, time_ref: str, event_details: Dict[str, Any]):
        """æ·»åŠ åŸºäºæ—¶é—´å‚è€ƒçš„æƒ…æ™¯è®°å¿†"""
        # è§£ææ—¶é—´å‚è€ƒ
        timestamp = self._parse_time_reference(time_ref)
        if not timestamp:
            print(f"æ— æ³•è§£ææ—¶é—´å‚è€ƒ: {time_ref}")
            return
        
        # åˆ›å»ºæ—¶é—´ç‚¹è®°å½•ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®ç»“æ„
        event_entry = {
            "id": str(uuid.uuid4()),
            "timestamp": timestamp,
            "datetime": datetime.fromtimestamp(timestamp).isoformat() if timestamp else datetime.now().isoformat(),
            "time_reference": time_ref,  # æ·»åŠ æ—¶é—´å‚è€ƒå­—æ®µ
            "interaction": {
                "user_message": event_details.get("user_message", ""),
                "ai_response": event_details.get("ai_response", ""),
                "emotional_insights": event_details.get("emotional_insights", {})
            },
            "activity": event_details.get("activity", "å…¶ä»–æ´»åŠ¨"),  # æ·»åŠ æ´»åŠ¨å­—æ®µ
            "summary": self._summarize_time_events([event_details])
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ—¶é—´ç‚¹çš„è®°å¿†
        existing_index = None
        for i, memory in enumerate(self.episodic_memory):
            if abs(memory["timestamp"] - timestamp) < 24 * 60 * 60:  # 24å°æ—¶å®¹å·®
                existing_index = i
                break
        
        if existing_index is not None:
            # åˆå¹¶äº‹ä»¶è¯¦æƒ…ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®ç»“æ„
            existing = self.episodic_memory[existing_index]
            
            # ç¡®ä¿existingæœ‰ç»Ÿä¸€çš„ç»“æ„
            if "interaction" not in existing:
                existing["interaction"] = {
                    "user_message": "",
                    "ai_response": "",
                    "emotional_insights": {}
                }
            
            # æ·»åŠ æ—¶é—´å‚è€ƒå­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if "time_reference" not in existing:
                existing["time_reference"] = time_ref
            
            # æ·»åŠ æ´»åŠ¨å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if "activity" not in existing:
                existing["activity"] = "å…¶ä»–æ´»åŠ¨"
            
            # æ·»åŠ æ–°çš„äº¤äº’è®°å½•åˆ°ç°æœ‰è®°å½•ä¸­
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿æŒå•ä¸ªinteractionï¼Œè€Œä¸æ˜¯detailsæ•°ç»„
            # å¦‚æœéœ€è¦è®°å½•å¤šä¸ªäº¤äº’ï¼Œå¯ä»¥è€ƒè™‘å…¶ä»–æ–¹å¼
            existing["interaction"] = {
                "user_message": event_details.get("user_message", ""),
                "ai_response": event_details.get("ai_response", ""),
                "emotional_insights": event_details.get("emotional_insights", {})
            }
            
            # æ›´æ–°æ´»åŠ¨ä¿¡æ¯
            existing["activity"] = event_details.get("activity", existing.get("activity", "å…¶ä»–æ´»åŠ¨"))
            
            # é‡æ–°ç”Ÿæˆæ‘˜è¦
            existing["summary"] = self._summarize_time_events([event_details])
        else:
            # æ·»åŠ æ–°çš„æ—¶é—´ç‚¹è®°å½•
            self.episodic_memory.append(event_entry)
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        self.save_memories()
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.collection:
            try:
                self.collection.add(
                    documents=[event_entry["summary"]],
                    metadatas=[{
                        "summary": event_entry["summary"],
                        "timestamp": event_entry["timestamp"],
                        "time_reference": event_entry["time_reference"],
                        "datetime": event_entry["datetime"]
                    }],
                    ids=[event_entry["id"]]
                )
            except Exception as e:
                print(f"Warning: Could not add to vector database: {e}")

    def _summarize_time_events(self, events: List[Dict[str, Any]]) -> str:
        """è‡ªåŠ¨æ€»ç»“æ—¶é—´ç‚¹äº‹ä»¶"""
        # ç®€å•å®ç°ï¼šè¿æ¥æ‰€æœ‰äº‹ä»¶çš„å…³é”®ä¿¡æ¯
        summary_parts = []
        for event in events:
            if "emotional_insights" in event and event["emotional_insights"].get("emotions"):
                summary_parts.append(f"è¡¨è¾¾äº†{', '.join(event['emotional_insights']['emotions'])}")
            if "activity" in event:
                summary_parts.append(f"è¿›è¡Œäº†{event['activity']}")
        
        return "ï¼›".join(summary_parts) if summary_parts else "å‘ç”Ÿäº†æŸäº›äº‹ä»¶"

    def update_semantic_memory(self, key: str, value: Any):
        """Update semantic memory with a key-value pair"""
        self.semantic_memory[key] = value
        self.save_memories()
    
    def get_working_memory_context(self) -> List[Dict[str, str]]:
        """Get the current working memory as context"""
        return [item["message"] for item in self.working_memory]
    
    def get_relevant_episodic_memories(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Get relevant episodic memories based on a query"""
        if self.collection:
            try:
                # Search in vector database
                results = self.collection.query(
                    query_texts=[query],
                    n_results=min(limit, len(self.episodic_memory)) if self.episodic_memory else 1
                )
                
                if results["metadatas"] and results["metadatas"][0]:
                    # å°†å­—ç¬¦ä¸²åŒ–çš„äº¤äº’ä¿¡æ¯è½¬æ¢å›å­—å…¸
                    metadatas = results["metadatas"][0]
                    for metadata in metadatas:
                        if "interaction" in metadata and isinstance(metadata["interaction"], str):
                            try:
                                metadata["interaction"] = json.loads(metadata["interaction"])
                            except:
                                pass  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸæ ·
                    return metadatas
            except Exception as e:
                print(f"Warning: Vector database query failed: {e}")
        
        # Fallback to returning most recent memories
        return self.episodic_memory[-limit:] if self.episodic_memory else []
    
    def get_user_profile(self) -> Dict[str, Any]:
        """Get the user profile from semantic memory"""
        return self.semantic_memory.get("user_profile", {})
    
    def reset_memory(self):
        """Reset all memory for the user"""
        self.working_memory = []
        self.episodic_memory = []
        self.semantic_memory = {}
        
        # Clear vector database collection
        if self.collection:
            try:
                self.collection.delete(ids=[mem["id"] for mem in self.episodic_memory])
            except Exception as e:
                print(f"Warning: Could not clear vector database: {e}")
        
        # Remove memory files
        for filename in ["semantic_memory.json", "episodic_memory.json"]:
            file_path = os.path.join(self.user_dir, filename)
            if os.path.exists(file_path):
                try:
                    os.remove(file_path)
                except Exception as e:
                    print(f"Warning: Could not remove {filename}: {e}")
    
    def _extract_time_reference(self, user_message: str) -> Optional[str]:
        """ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æ—¶é—´å‚è€ƒ"""
        import re
        
        # æ—¶é—´è¡¨è¾¾å¼æ¨¡å¼
        patterns = [
            r'æ˜¨å¤©', r'ä»Šå¤©', r'æ˜å¤©', r'å‰å¤©', r'å¤§å‰å¤©',
            r'ä¸Šå‘¨', r'è¿™å‘¨', r'ä¸‹å‘¨',
            r'ä¸Šä¸ªæœˆ', r'è¿™ä¸ªæœˆ', r'ä¸‹ä¸ªæœˆ',
            r'å»å¹´', r'ä»Šå¹´', r'æ˜å¹´',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # YYYYå¹´MMæœˆDDæ—¥
            r'(\d{1,2})æœˆ(\d{1,2})æ—¥',           # MMæœˆDDæ—¥
            r'(\d{4})-(\d{1,2})-(\d{1,2})',      # YYYY-MM-DD
            r'(\d{1,2})/(\d{1,2})/(\d{4})',      # MM/DD/YYYY
            r'æš‘å‡', r'å¯’å‡', r'æ˜¥èŠ‚'  # ç‰¹æ®Šæ—¶é—´ç‚¹
        ]
        
        for pattern in patterns:
            match = re.search(pattern, user_message)
            if match:
                return match.group(0)
        
        return None

    def _parse_time_reference(self, time_ref: str) -> Optional[float]:
        """å°†æ—¶é—´å‚è€ƒè§£æä¸ºæ—¶é—´æˆ³"""
        from datetime import datetime, timedelta
        import re
        
        now = datetime.now()
        
        # ç›¸å¯¹æ—¶é—´
        if time_ref == 'æ˜¨å¤©':
            return (now - timedelta(days=1)).timestamp()
        elif time_ref == 'ä»Šå¤©':
            return now.timestamp()
        elif time_ref == 'å‰å¤©':
            return (now - timedelta(days=2)).timestamp()
        elif time_ref == 'å»å¹´':
            return (now - timedelta(days=365)).timestamp()
        elif time_ref == 'ä»Šå¹´':
            return now.timestamp()  # ä»Šå¹´åº”è¯¥è¿”å›å¹´åˆçš„æ—¶é—´æˆ³
        elif time_ref == 'æš‘å‡':
            # å‡è®¾æš‘å‡æ˜¯7-8æœˆ
            return now.replace(month=7, day=1).timestamp()
        elif time_ref == 'å¯’å‡':
            # å‡è®¾å¯’å‡æ˜¯1-2æœˆ
            return now.replace(month=1, day=1).timestamp()
        
        # ç»å¯¹æ—¶é—´
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{1,2})/(\d{1,2})/(\d{4})',
        ]
        
        for pattern in date_patterns:
            match = re.match(pattern, time_ref)
            if match:
                try:
                    if pattern == date_patterns[0]:  # YYYYå¹´MMæœˆDDæ—¥
                        year, month, day = map(int, match.groups())
                    elif pattern == date_patterns[1]:  # YYYY-MM-DD
                        year, month, day = map(int, match.groups())
                    else:  # MM/DD/YYYY
                        month, day, year = map(int, match.groups())
                    
                    date = datetime(year, month, day)
                    return date.timestamp()
                except:
                    continue
        
        return None

    def get_episodic_memory_by_time(self, time_ref: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®æ—¶é—´å‚è€ƒè·å–æƒ…æ™¯è®°å¿†"""
        timestamp = self._parse_time_reference(time_ref)
        if not timestamp:
            return None
        
        # æŸ¥æ‰¾æœ€æ¥è¿‘çš„æ—¶é—´ç‚¹ï¼ˆå…è®¸ä¸€å®šçš„æ—¶é—´è¯¯å·®ï¼‰
        tolerance = 24 * 60 * 60  # 24å°æ—¶å®¹å·®
        best_match = None
        best_diff = float('inf')
        
        for memory in self.episodic_memory:
            # æ£€æŸ¥è®°å¿†æ˜¯å¦æœ‰æ—¶é—´å‚è€ƒå­—æ®µå¹¶ä¸”åŒ¹é…
            if "time_reference" in memory and memory["time_reference"] == time_ref:
                return memory
            
            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨æ—¶é—´æˆ³åŒ¹é…
            diff = abs(memory["timestamp"] - timestamp)
            if diff <= tolerance and diff < best_diff:
                best_match = memory
                best_diff = diff
        
        return best_match

class AIPsychologist:
    """Main AI Psychologist class with long-term memory capabilities"""
    
    def __init__(self, user_id: str = "default_user"):
        self.user_id = user_id
        self.llm_client = LLMClient()  # ä½¿ç”¨ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯
        self.memory_system = MemorySystem(user_id)
        
        # Initialize with a default personality
        self.personality = "empathetic"
        
        # Initialize user profile if it doesn't exist
        if not self.memory_system.get_user_profile():
            self.memory_system.update_semantic_memory("user_profile", {
                "preferences": {},
                "psychological_history": [],
                "personality_insights": {}
            })
    
    def _extract_emotional_insights(self, user_message: str) -> Dict[str, Any]:
        """Extract emotional insights from user message (simplified)"""
        # In a real implementation, this would use sentiment analysis and NLP
        insights = {
            "emotions": [],
            "topics": [],
            "intensity": 0
        }
        
        # Simple keyword-based emotion detection
        emotions = {
            "sadness": ["sad", "depressed", "unhappy", "down", "blue", "depression", "éš¾è¿‡", "æ²®ä¸§"],
            "anxiety": ["anxious", "worried", "nervous", "stressed", "concerned", "panic", "ç„¦è™‘", "æ‹…å¿ƒ"],
            "anger": ["angry", "mad", "frustrated", "irritated", "annoyed", "rage", "æ„¤æ€’", "ç”Ÿæ°”"],
            "happiness": ["happy", "joy", "pleased", "delighted", "excited", "glad", "é«˜å…´", "å¿«ä¹"],
            "loneliness": ["lonely", "alone", "isolated", "by myself", "solitude", "å­¤ç‹¬"]
        }
        
        user_message_lower = user_message.lower()
        for emotion, keywords in emotions.items():
            for keyword in keywords:
                if keyword in user_message_lower:
                    insights["emotions"].append(emotion)
                    insights["intensity"] = max(insights["intensity"], len(keyword))
        
        return insights
    
    def _process_time_reference(self, user_message: str) -> Optional[str]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ä¸­çš„æ—¶é—´å‚è€ƒ"""
        time_ref = self.memory_system._extract_time_reference(user_message)
        if time_ref:
            # æ£€ç´¢ç›¸å…³æƒ…æ™¯è®°å¿†
            episodic_memory = self.memory_system.get_episodic_memory_by_time(time_ref)
            if episodic_memory:
                # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®ç»“æ„å¤„ç†
                try:
                    # è·å–æ‘˜è¦ä¿¡æ¯
                    summary = episodic_memory.get("summary", "å‘ç”Ÿäº†æŸäº›äº‹ä»¶")
                    activity = episodic_memory.get("activity", "è¿›è¡Œäº†æŸäº›æ´»åŠ¨")
                    
                    return f"æ ¹æ®æˆ‘ä»¬ä¹‹å‰åœ¨{time_ref}çš„å¯¹è¯è®°å½•ï¼š{summary}"
                except Exception as e:
                    # å¦‚æœå‡ºç°ä»»ä½•é”™è¯¯ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶è¿”å›é»˜è®¤å“åº”
                    print(f"å¤„ç†æ—¶é—´å‚è€ƒæ—¶å‡ºé”™: {e}")
                    return None
        
        return None

    def _build_context(self, user_message: str) -> List[Dict[str, str]]:
        """Build context for the LLM using multi-layered memory"""
        # Start with working memory (recent conversation)
        context = self.memory_system.get_working_memory_context()
        
        # Add the main system prompt with current time
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        main_system_prompt = f"ä½ æ˜¯ä¸€ä½AIå¿ƒç†å­¦å®¶ï¼Œè¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è§£å†³ç”¨æˆ·çš„å¿ƒç†é—®é¢˜ï¼Œå¿…é¡»éµå®ˆå®‰å…¨åŸåˆ™ï¼Œç°åœ¨çš„æ—¶é—´æ˜¯{current_time}ï¼Œä½ æ˜¯å…·æœ‰é•¿æœŸè®°å¿†çš„ï¼ˆç³»ç»Ÿä¼šç»™ä½ ï¼‰ã€‚"
        
        context.insert(0, {
            "role": "system",
            "content": main_system_prompt
        })
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´å‚è€ƒ
        time_context = self._process_time_reference(user_message)
        if time_context:
            context.insert(1, {  # æ’å…¥åˆ°ä¸»ç³»ç»Ÿæç¤ºä¹‹å
                "role": "system",
                "content": f"å†å²èƒŒæ™¯: {time_context}"
            })
        
        # Add relevant therapeutic techniques based on user message
        relevant_techniques = procedural_memory.get_relevant_techniques(user_message)
        if relevant_techniques:
            technique_info = "å¯ç”¨çš„æ²»ç–—æŠ€æœ¯:\n"
            for name, technique in list(relevant_techniques.items())[:3]:  # Limit to 3 techniques
                technique_info += procedural_memory.format_technique_for_prompt(name, technique) + "\n"
            
            context.insert(1, {  # æ’å…¥åˆ°ä¸»ç³»ç»Ÿæç¤ºä¹‹å
                "role": "system",
                "content": f"æ²»ç–—æŠ€æœ¯å‚è€ƒ:\n{technique_info}"
            })
        
        # Add relevant episodic memories
        relevant_memories = self.memory_system.get_relevant_episodic_memories(user_message)
        if relevant_memories:
            memory_summary = "ç›¸å…³çš„è¿‡å¾€å¯¹è¯:\n"
            for mem in relevant_memories[-3:]:  # Last 3 memories
                # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®ç»“æ„å¤„ç†
                if "interaction" in mem:
                    # æ–°çš„ç»Ÿä¸€ç»“æ„
                    summary = mem.get("summary", "å¯¹è¯")
                else:
                    # æ—§çš„ç»“æ„æˆ–å…¶ä»–ç»“æ„
                    summary = mem.get("summary", "å¯¹è¯")
                
                memory_summary += f"- {summary}\n"
            
            context.insert(1, {  # æ’å…¥åˆ°ä¸»ç³»ç»Ÿæç¤ºä¹‹å
                "role": "system",
                "content": f"å†å²èƒŒæ™¯: {memory_summary}"
            })
        
        # Add user profile information
        user_profile = self.memory_system.get_user_profile()
        if user_profile:
            profile_info = f"ç”¨æˆ·æ¡£æ¡ˆ: {json.dumps(user_profile, ensure_ascii=False)}"
            context.insert(1, {  # æ’å…¥åˆ°ä¸»ç³»ç»Ÿæç¤ºä¹‹å
                "role": "system",
                "content": profile_info
            })
        
        # Add the current user message
        context.append({
            "role": "user",
            "content": user_message
        })
        
        return context

    def _update_memory(self, user_message: str, ai_response: str):
        """Update memory systems with the current interaction"""
        # Add to working memory
        self.memory_system.add_working_memory({
            "role": "user",
            "content": user_message
        })
        
        self.memory_system.add_working_memory({
            "role": "assistant",
            "content": ai_response
        })
        
        # Extract and store emotional insights
        emotional_insights = self._extract_emotional_insights(user_message)
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æåˆ°ç‰¹å®šæ—¶é—´ç‚¹
        time_ref = self.memory_system._extract_time_reference(user_message)
        if time_ref:
            # æå–æ´»åŠ¨ä¿¡æ¯
            activity_info = {
                "user_message": user_message,
                "ai_response": ai_response,
                "emotional_insights": emotional_insights,
                "activity": self._extract_activity(user_message)
            }
            
            # æ·»åŠ åˆ°åŸºäºæ—¶é—´çš„æƒ…æ™¯è®°å¿†
            self.memory_system.add_time_based_episodic_memory(time_ref, activity_info)
        else:
            # æ·»åŠ åˆ°æ™®é€šæƒ…æ™¯è®°å¿†
            self.memory_system.add_episodic_memory({
                "interaction": {
                    "user_message": user_message,
                    "ai_response": ai_response,
                    "emotional_insights": emotional_insights
                },
                "summary": f"ç”¨æˆ·è¡¨è¾¾äº† {', '.join(emotional_insights['emotions']) if emotional_insights['emotions'] else 'æ„Ÿå—'}"
            })
        
        # Update semantic memory with user preferences and insights
        user_profile = self.memory_system.get_user_profile()
        if emotional_insights["emotions"]:
            # Update personality insights
            personality_insights = user_profile.get("personality_insights", {})
            for emotion in emotional_insights["emotions"]:
                personality_insights[emotion] = personality_insights.get(emotion, 0) + 1
            
            user_profile["personality_insights"] = personality_insights
        
        # æå–å¹¶å­˜å‚¨ç”¨æˆ·åå¥½ä¿¡æ¯
        self._extract_and_store_preferences(user_message, user_profile)
        
        # æå–å¹¶å­˜å‚¨ç”¨æˆ·å…´è¶£å’Œå…³æ³¨ç‚¹
        self._extract_and_store_interests(user_message, user_profile)
        
        self.memory_system.update_semantic_memory("user_profile", user_profile)

    def _extract_and_store_preferences(self, user_message: str, user_profile: Dict[str, Any]):
        """æå–å¹¶å­˜å‚¨ç”¨æˆ·åå¥½ä¿¡æ¯"""
        # ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯æå–åå¥½
        preferences = user_profile.get("preferences", {})
        
        # é€šä¿¡åå¥½
        if any(word in user_message.lower() for word in ["æ–‡å­—", "æ‰“å­—", "èŠå¤©"]):
            preferences["communication_style"] = "text"
        elif any(word in user_message.lower() for word in ["è¯­éŸ³", "è¯´è¯", "è®²è¯"]):
            preferences["communication_style"] = "voice"
            
        # æ—¶é—´åå¥½
        if any(word in user_message.lower() for word in ["æ™šä¸Š", "å¤œé—´", "æ·±å¤œ"]):
            preferences["preferred_time"] = "evening"
        elif any(word in user_message.lower() for word in ["æ—©ä¸Š", "ä¸Šåˆ", "æ—©æ™¨"]):
            preferences["preferred_time"] = "morning"
            
        # è¯é¢˜åå¥½
        topic_keywords = {
            "career": ["å·¥ä½œ", "èŒä¸š", "é¢è¯•", "å‡èŒ", "å®ä¹ "],
            "relationships": ["æœ‹å‹", "å®¶äºº", "æ‹äºº", "å…³ç³»", "ç¤¾äº¤"],
            "health": ["å¥åº·", "é”»ç‚¼", "è¿åŠ¨", "èº«ä½“", "ç¡çœ "],
            "learning": ["å­¦ä¹ ", "çŸ¥è¯†", "è¯»ä¹¦", "æ•™è‚²", "æŠ€èƒ½"]
        }
        
        user_message_lower = user_message.lower()
        for topic, keywords in topic_keywords.items():
            if any(keyword in user_message_lower for keyword in keywords):
                current_count = preferences.get(f"interest_{topic}", 0)
                preferences[f"interest_{topic}"] = current_count + 1
        
        user_profile["preferences"] = preferences

    def _extract_and_store_interests(self, user_message: str, user_profile: Dict[str, Any]):
        """æå–å¹¶å­˜å‚¨ç”¨æˆ·å…´è¶£å’Œå…³æ³¨ç‚¹"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å…´è¶£æå–é€»è¾‘
        # ç›®å‰åªæ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹
        
        psychological_history = user_profile.get("psychological_history", [])
        
        # åŸºäºæƒ…ç»ªå’Œè¯é¢˜çš„ç®€å•å†å²è®°å½•
        if "å‹åŠ›" in user_message or "ç„¦è™‘" in user_message:
            psychological_history.append({
                "concern": "stress_and_anxiety",
                "timestamp": time.time(),
                "context": user_message[:100] + "..." if len(user_message) > 100 else user_message
            })
        
        if "ç¡çœ " in user_message or "å¤±çœ " in user_message:
            psychological_history.append({
                "concern": "sleep_issues",
                "timestamp": time.time(),
                "context": user_message[:100] + "..." if len(user_message) > 100 else user_message
            })
            
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(psychological_history) > 50:  # æœ€å¤šä¿å­˜50æ¡è®°å½•
            psychological_history = psychological_history[-50:]
            
        user_profile["psychological_history"] = psychological_history

    def _extract_activity(self, user_message: str) -> str:
        """ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æ´»åŠ¨ä¿¡æ¯"""
        # ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯æå–æ´»åŠ¨
        activities = {
            "å®ä¹ ": ["å®ä¹ ", "å·¥ä½œ", "ä¸Šç­"],
            "å­¦ä¹ ": ["å­¦ä¹ ", "å¤ä¹ ", "ä¸Šè¯¾", "è€ƒè¯•"],
            "æ—…è¡Œ": ["æ—…è¡Œ", "æ—…æ¸¸", "æ¸¸ç©"],
            "ä¼‘æ¯": ["ä¼‘æ¯", "æ”¾æ¾", "ç¡è§‰"],
            "è¿åŠ¨": ["è¿åŠ¨", "è·‘æ­¥", "å¥èº«"],
            "ç¤¾äº¤": ["èšä¼š", "æœ‹å‹", "èŠå¤©"]
        }
        
        user_message_lower = user_message.lower()
        for activity, keywords in activities.items():
            for keyword in keywords:
                if keyword in user_message_lower:
                    return activity
        
        return "å…¶ä»–æ´»åŠ¨"

    def chat(self, user_message: str) -> str:
        """Process a user message and generate a response"""
        # Build context using multi-layered memory
        context = self._build_context(user_message)
        
        # Get response from LLM
        response = self.llm_client.chat_completion(context)
        ai_response = response["choices"][0]["message"]["content"]
        
        # Update memory with this interaction
        self._update_memory(user_message, ai_response)
        
        return ai_response
    
    def reset_memory(self):
        """Reset all memory for the user"""
        self.memory_system.reset_memory()


# Example usage
if __name__ == "__main__":
    # Create an instance of the AI Psychologist
    psychologist = AIPsychologist("test_user")
    
    # Example conversation
    print("AIå¿ƒç†å­¦å®¶æ¼”ç¤º")
    print("=" * 30)
    
    test_messages = [
        "æˆ‘æœ€è¿‘æ„Ÿåˆ°å¾ˆç„¦è™‘ã€‚",
        "ä¸»è¦æ˜¯å› ä¸ºå³å°†åˆ°æ¥çš„å·¥ä½œé¢è¯•ã€‚",
        "æˆ‘æ‹…å¿ƒè¡¨ç°ä¸å¥½ä¼šè®©å®¶äººå¤±æœ›ã€‚"
    ]
    
    for message in test_messages:
        response = psychologist.chat(message)
        print(f"ä½ : {message}")
        print(f"AIå¿ƒç†å­¦å®¶: {response}\n")